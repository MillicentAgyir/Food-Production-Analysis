{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52bde21b",
   "metadata": {},
   "source": [
    "## PHASE 1 : BUSINESS UNDERSTANDING\n",
    "\n",
    "# Problem statement\n",
    "\n",
    "Identify which foods and lifecycle stages (farm, processing, transport, etc.) drive the largest environmental burdens, and recommend the highest-leverage actions (dietary swaps, sourcing policies, logistics/packaging changes) to lower total impact without undermining nutrition or cost.\n",
    "\n",
    "\n",
    "# Stakeholders & decisions\n",
    "\n",
    "- Policy & NGOs: dietary guidance, incentives for lower-impact foods, water-scarcity risk management. \n",
    "\n",
    "- Procurement & Retail: product mix, supplier selection, transport/packaging optimization.\n",
    "\n",
    "- Producers/Farmers: practice changes (feed, fertilizer, irrigation efficiency).\n",
    "\n",
    "- Consumers: informed swaps toward lower-impact alternatives.\n",
    "\n",
    "# Success metrics (KPIs)\n",
    "\n",
    "- GHG intensity (kg CO₂-eq per kg product; and optionally per 1000 kcal / per 100 g protein for fair comparisons across food types).\n",
    "\n",
    "- Water footprint (freshwater withdrawals; scarcity-weighted water use).\n",
    "\n",
    "- Land use & land-use change contributions.\n",
    "\n",
    "- Stage contributions (% share from farm/feed/processing/transport/packaging/retail).\n",
    "\n",
    "\n",
    "# Business Questions\n",
    "These are the key business questions to be answered by the end of the project:\n",
    "\n",
    "1. Which foods are highest/lowest impact by GHG per kg? By kcal? By 100 g protein? \n",
    "\n",
    "2. Which lifecycle stages dominate impacts for each food (e.g., farm vs transport vs packaging)? \n",
    "\n",
    "\n",
    "3. Top leverage points: which 8–10 foods account for ~80% of total GHG (Pareto) and what stage drives each?\n",
    "\n",
    "4. Water risk: which foods have extreme scarcity-weighted water use, and where do withdrawals cluster?\n",
    "\n",
    "5. Dietary swaps: what realistic substitutions (e.g., beef → poultry/legumes; dairy → plant milks) yield the largest impact reduction per serving of protein/kcal?\n",
    "\n",
    "6. Transport & packaging sensitivity: for which foods are these stages non-trivial (i.e., >10–15%)?\n",
    "\n",
    "7. Consistency trade-offs: do lower-GHG foods sometimes have higher water or land footprints? What’s the recommended balance?\n",
    "\n",
    "8. Scenario impact: if a retailer shifts X% of sales from high- to medium-impact foods, what is the projected GHG/water reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af14ea46",
   "metadata": {},
   "source": [
    "# PHASE 2: DATA UNDERSTANDING\n",
    "The objective of this phase is to load the dataset, understand its structure, and perform initial exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4caf44",
   "metadata": {},
   "source": [
    "A. Loading and Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "570d290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CSV files collected: 296\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_root = './data'\n",
    "csv_files = []\n",
    "\n",
    "# Step 1: Unzip all files (if needed)\n",
    "for root, _, files in os.walk(data_root):\n",
    "    for file in files:\n",
    "        if file.endswith('.zip'):\n",
    "            zip_path = os.path.join(root, file)\n",
    "            extract_path = os.path.splitext(zip_path)[0]\n",
    "            if not os.path.exists(extract_path):  # Avoid re-extracting\n",
    "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(extract_path)\n",
    "\n",
    "# Step 2: Collect CSVs\n",
    "for root, _, files in os.walk(data_root):\n",
    "    for file in files:\n",
    "        if file.lower().endswith('.csv') and '__MACOSX' not in root and not file.startswith('._'):\n",
    "            csv_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Total CSV files collected: {len(csv_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3be9fa",
   "metadata": {},
   "source": [
    "Before merging, I'm going to validate column structure across all 60 files to ensure that there are no missing or extra columns, column order is consistent so that I know early if any files need fixing:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "471b858d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 6 unique column structures.\n",
      "\n",
      "Structure 1: 97 file(s)\n",
      "Columns: ('TRDTYPE', 'USASTATE', 'DEPE', 'DISAGMOT', 'MEXSTATE', 'CANPROV', 'COUNTRY', 'VALUE', 'SHIPWT', 'FREIGHT_CHARGES') ...\n",
      "Sample file: ./data\\2020\\April2020TransBorderRawData\\Apr 2020\\dot1_0420.csv\n",
      "\n",
      "Structure 2: 97 file(s)\n",
      "Columns: ('TRDTYPE', 'USASTATE', 'COMMODITY2', 'DISAGMOT', 'MEXSTATE', 'CANPROV', 'COUNTRY', 'VALUE', 'SHIPWT', 'FREIGHT_CHARGES') ...\n",
      "Sample file: ./data\\2020\\April2020TransBorderRawData\\Apr 2020\\dot2_0420.csv\n",
      "\n",
      "Structure 3: 96 file(s)\n",
      "Columns: ('TRDTYPE', 'DEPE', 'COMMODITY2', 'DISAGMOT', 'COUNTRY', 'VALUE', 'SHIPWT', 'FREIGHT_CHARGES', 'DF', 'CONTCODE') ...\n",
      "Sample file: ./data\\2020\\April2020TransBorderRawData\\Apr 2020\\dot3_0420.csv\n",
      "\n",
      "Structure 4: 2 file(s)\n",
      "Columns: ('TRDTYPE', 'USASTATE', 'DEPE', 'DISAGMOT', 'MEXSTATE', 'CANPROV', 'COUNTRY', 'DF', 'CONTCODE', 'YEAR') ...\n",
      "Sample file: ./data\\2021\\July-to-Dec-2021\\New folder\\Dec 2021\\dot1_2021.csv\n",
      "\n",
      "Structure 5: 2 file(s)\n",
      "Columns: ('TRDTYPE', 'USASTATE', 'COMMODITY2', 'DISAGMOT', 'MEXSTATE', 'CANPROV', 'COUNTRY', 'DF', 'CONTCODE', 'YEAR') ...\n",
      "Sample file: ./data\\2021\\July-to-Dec-2021\\New folder\\Dec 2021\\dot2_2021.csv\n",
      "\n",
      "Structure 6: 2 file(s)\n",
      "Columns: ('TRDTYPE', 'DEPE', 'COMMODITY2', 'DISAGMOT', 'COUNTRY', 'DF', 'CONTCODE', 'YEAR', 'VALUE', 'SHIPWT') ...\n",
      "Sample file: ./data\\2021\\July-to-Dec-2021\\New folder\\Dec 2021\\dot3_2021.csv\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Step 4: Classify CSVs by column structure\n",
    "def group_csvs_by_structure(file_list):\n",
    "    structure_map = defaultdict(list)\n",
    "\n",
    "    for file in file_list:\n",
    "        try:\n",
    "            df_sample = pd.read_csv(file, nrows=5, dtype=str, low_memory=False, encoding='utf-8')\n",
    "            col_tuple = tuple(df_sample.columns.str.upper().str.strip())\n",
    "            structure_map[col_tuple].append(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped {file}: {e}\")\n",
    "\n",
    "    # Report\n",
    "    print(f\"\\nFound {len(structure_map)} unique column structures.\")\n",
    "    for i, (cols, files) in enumerate(structure_map.items(), 1):\n",
    "        print(f\"\\nStructure {i}: {len(files)} file(s)\")\n",
    "        print(\"Columns:\", cols[:10], \"...\")  # Show only first 10 columns\n",
    "        print(\"Sample file:\", files[0])\n",
    "    return structure_map\n",
    "\n",
    "structure_map = group_csvs_by_structure(csv_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4367c015",
   "metadata": {},
   "source": [
    "This output shows that there are 6 different/unique columns structure so I'm going to merge each structure separately,and create 3 master DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c0e018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging 97 files for dot1...\n",
      "dot1 merged: shape = (8211492, 15)\n",
      "\n",
      "Merging 97 files for dot2...\n",
      "dot2 merged: shape = (22505735, 15)\n",
      "\n",
      "Merging 96 files for dot3...\n",
      "dot3 merged: shape = (5133293, 13)\n"
     ]
    }
   ],
   "source": [
    "# Map structure indices to group names manually after inspecting them\n",
    "dot1_files = structure_map[list(structure_map.keys())[0]]\n",
    "dot2_files = structure_map[list(structure_map.keys())[1]]\n",
    "dot3_files = structure_map[list(structure_map.keys())[2]]\n",
    "\n",
    "# If you noticed other variants (e.g. newer versions), you can merge them in too:\n",
    "# dot1_files += structure_map[list(structure_map.keys())[3]]\n",
    "\n",
    "def merge_csv_files(file_list, group_name):\n",
    "    merged = []\n",
    "    print(f\"\\nMerging {len(file_list)} files for {group_name}...\")\n",
    "\n",
    "    for file in file_list:\n",
    "        try:\n",
    "            chunk = pd.read_csv(file, dtype=str, low_memory=False, encoding='utf-8')\n",
    "            chunk.columns = chunk.columns.str.upper().str.strip()\n",
    "            chunk['SOURCE_FILE'] = os.path.basename(file)\n",
    "            merged.append(chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {file}: {e}\")\n",
    "    \n",
    "    df_merged = pd.concat(merged, ignore_index=True)\n",
    "    print(f\"{group_name} merged: shape = {df_merged.shape}\")\n",
    "    return df_merged\n",
    "\n",
    "dot1 = merge_csv_files(dot1_files, \"dot1\")\n",
    "dot2 = merge_csv_files(dot2_files, \"dot2\")\n",
    "dot3 = merge_csv_files(dot3_files, \"dot3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ceb6b",
   "metadata": {},
   "source": [
    "Save each of the set as csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2874e61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: merged_datasets\\dot1_merged.csv — Shape: (8211492, 15)\n",
      " Saved: merged_datasets\\dot2_merged.csv — Shape: (22505735, 15)\n",
      " Saved: merged_datasets\\dot3_merged.csv — Shape: (5133293, 13)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create output folder (if not existing)\n",
    "output_dir = \"merged_datasets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to merge and save each group\n",
    "def merge_and_save(file_list, group_name):\n",
    "    merged_data = []\n",
    "    for file in file_list:\n",
    "        try:\n",
    "            df = pd.read_csv(file, dtype=str, low_memory=False, encoding='utf-8')\n",
    "            df.columns = df.columns.str.upper().str.strip()\n",
    "            df[\"SOURCE_FILE\"] = os.path.basename(file)\n",
    "            merged_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\" Failed to read {file}: {e}\")\n",
    "\n",
    "    if merged_data:\n",
    "        merged_df = pd.concat(merged_data, ignore_index=True)\n",
    "        output_path = os.path.join(output_dir, f\"{group_name}_merged.csv\")\n",
    "        merged_df.to_csv(output_path, index=False)\n",
    "        print(f\" Saved: {output_path} — Shape: {merged_df.shape}\")\n",
    "    else:\n",
    "        print(f\" No valid files to merge for {group_name}\")\n",
    "\n",
    "# Merge and save each group\n",
    "merge_and_save(dot1_files, \"dot1\")\n",
    "merge_and_save(dot2_files, \"dot2\")\n",
    "merge_and_save(dot3_files, \"dot3\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
